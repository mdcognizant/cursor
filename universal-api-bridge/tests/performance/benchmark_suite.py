#!/usr/bin/env python3
"""Comprehensive performance testing suite for Universal API Bridge."""

import asyncio
import time
import statistics
import json
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import aiohttp
import requests
import psutil
import numpy as np

from universal_api_bridge import UniversalBridge, BridgeConfig
from universal_api_bridge.config import create_massive_scale_config, create_cluster_from_endpoints


@dataclass
class BenchmarkResult:
    """Results from a benchmark test."""
    test_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    requests_per_second: float
    avg_latency: float
    p95_latency: float
    p99_latency: float
    memory_usage_mb: float
    cpu_usage_percent: float
    errors: List[str] = field(default_factory=list)


class PerformanceBenchmark:
    """High-performance benchmark suite for Universal API Bridge."""
    
    def __init__(self, bridge_port: int = 8000):
        self.bridge_port = bridge_port
        self.bridge_url = f"http://localhost:{bridge_port}"
        self.bridge: UniversalBridge = None
        self.results: List[BenchmarkResult] = []
        
    async def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """Run the complete benchmark suite."""
        print("ğŸš€ Starting Universal API Bridge Performance Benchmark Suite")
        print("=" * 70)
        
        # Test scenarios
        scenarios = [
            ("baseline_rest", self.benchmark_baseline_rest, 1000, 10),
            ("universal_bridge_1k", self.benchmark_universal_bridge, 1000, 10), 
            ("universal_bridge_10k", self.benchmark_universal_bridge, 10000, 50),
            ("universal_bridge_100k", self.benchmark_universal_bridge, 100000, 100),
            ("massive_scale_10k_services", self.benchmark_massive_scale, 10000, 20),
            ("streaming_performance", self.benchmark_streaming, 5000, 30),
            ("concurrent_load", self.benchmark_concurrent_load, 20000, 100),
            ("memory_efficiency", self.benchmark_memory_efficiency, 50000, 60),
        ]
        
        for test_name, test_func, requests, duration in scenarios:
            print(f"\nğŸ“Š Running {test_name}...")
            try:
                result = await test_func(requests, duration)
                self.results.append(result)
                self._print_result(result)
            except Exception as e:
                print(f"âŒ Test {test_name} failed: {e}")
                error_result = BenchmarkResult(
                    test_name=test_name,
                    total_requests=0,
                    successful_requests=0,
                    failed_requests=requests,
                    total_time=0,
                    requests_per_second=0,
                    avg_latency=0,
                    p95_latency=0,
                    p99_latency=0,
                    memory_usage_mb=0,
                    cpu_usage_percent=0,
                    errors=[str(e)]
                )
                self.results.append(error_result)
        
        # Generate comprehensive report
        return self._generate_comprehensive_report()
    
    async def benchmark_baseline_rest(self, num_requests: int, duration: int) -> BenchmarkResult:
        """Benchmark baseline REST API performance."""
        latencies = []
        errors = []
        successful = 0
        failed = 0
        
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        # Simple HTTP server for baseline
        async with aiohttp.ClientSession() as session:
            tasks = []
            for _ in range(num_requests):
                task = self._make_baseline_request(session)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    failed += 1
                    errors.append(str(result))
                else:
                    successful += 1
                    latencies.append(result)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        total_time = end_time - start_time
        
        return BenchmarkResult(
            test_name="baseline_rest",
            total_requests=num_requests,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            requests_per_second=successful / total_time if total_time > 0 else 0,
            avg_latency=statistics.mean(latencies) if latencies else 0,
            p95_latency=np.percentile(latencies, 95) if latencies else 0,
            p99_latency=np.percentile(latencies, 99) if latencies else 0,
            memory_usage_mb=end_memory - start_memory,
            cpu_usage_percent=psutil.cpu_percent(interval=1),
            errors=errors[:10]  # Keep only first 10 errors
        )
    
    async def benchmark_universal_bridge(self, num_requests: int, duration: int) -> BenchmarkResult:
        """Benchmark Universal API Bridge performance."""
        # Setup bridge for testing
        await self._setup_test_bridge(num_services=min(100, num_requests // 100))
        
        latencies = []
        errors = []
        successful = 0
        failed = 0
        
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        # Test through Universal Bridge
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(num_requests):
                service_name = f"test-service-{i % 10}"  # Distribute across services
                task = self._make_bridge_request(session, service_name, i)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    failed += 1
                    errors.append(str(result))
                else:
                    successful += 1
                    latencies.append(result)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        total_time = end_time - start_time
        
        await self._cleanup_test_bridge()
        
        return BenchmarkResult(
            test_name="universal_bridge",
            total_requests=num_requests,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            requests_per_second=successful / total_time if total_time > 0 else 0,
            avg_latency=statistics.mean(latencies) if latencies else 0,
            p95_latency=np.percentile(latencies, 95) if latencies else 0,
            p99_latency=np.percentile(latencies, 99) if latencies else 0,
            memory_usage_mb=end_memory - start_memory,
            cpu_usage_percent=psutil.cpu_percent(interval=1),
            errors=errors[:10]
        )
    
    async def benchmark_massive_scale(self, num_services: int, duration: int) -> BenchmarkResult:
        """Benchmark with massive number of services (10K+)."""
        print(f"   ğŸ”§ Setting up {num_services} services...")
        
        # Create massive scale configuration
        config = create_massive_scale_config(num_services)
        
        # Setup bridge with many services
        await self._setup_massive_scale_bridge(num_services)
        
        # Test with distributed load across all services
        num_requests = num_services * 2  # 2 requests per service
        
        latencies = []
        errors = []
        successful = 0
        failed = 0
        
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(num_requests):
                service_name = f"service-{i % num_services}"
                task = self._make_bridge_request(session, service_name, i)
                tasks.append(task)
            
            # Execute in batches to avoid overwhelming the system
            batch_size = 1000
            for i in range(0, len(tasks), batch_size):
                batch = tasks[i:i + batch_size]
                results = await asyncio.gather(*batch, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        failed += 1
                        errors.append(str(result))
                    else:
                        successful += 1
                        latencies.append(result)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        total_time = end_time - start_time
        
        await self._cleanup_test_bridge()
        
        return BenchmarkResult(
            test_name="massive_scale",
            total_requests=num_requests,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            requests_per_second=successful / total_time if total_time > 0 else 0,
            avg_latency=statistics.mean(latencies) if latencies else 0,
            p95_latency=np.percentile(latencies, 95) if latencies else 0,
            p99_latency=np.percentile(latencies, 99) if latencies else 0,
            memory_usage_mb=end_memory - start_memory,
            cpu_usage_percent=psutil.cpu_percent(interval=1),
            errors=errors[:10]
        )
    
    async def benchmark_streaming(self, num_streams: int, duration: int) -> BenchmarkResult:
        """Benchmark streaming performance."""
        # This would test streaming gRPC through the bridge
        # For now, simulate streaming behavior
        
        latencies = []
        successful = 0
        failed = 0
        
        start_time = time.time()
        
        # Simulate streaming requests
        for i in range(num_streams):
            try:
                # Simulate stream processing
                await asyncio.sleep(0.001)  # 1ms per stream message
                latencies.append(0.001)
                successful += 1
            except Exception as e:
                failed += 1
        
        end_time = time.time()
        total_time = end_time - start_time
        
        return BenchmarkResult(
            test_name="streaming",
            total_requests=num_streams,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            requests_per_second=successful / total_time if total_time > 0 else 0,
            avg_latency=statistics.mean(latencies) if latencies else 0,
            p95_latency=np.percentile(latencies, 95) if latencies else 0,
            p99_latency=np.percentile(latencies, 99) if latencies else 0,
            memory_usage_mb=0,
            cpu_usage_percent=psutil.cpu_percent(interval=1),
            errors=[]
        )
    
    async def benchmark_concurrent_load(self, num_requests: int, duration: int) -> BenchmarkResult:
        """Benchmark high concurrent load."""
        await self._setup_test_bridge(num_services=50)
        
        latencies = []
        errors = []
        successful = 0
        failed = 0
        
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        # High concurrency test
        semaphore = asyncio.Semaphore(1000)  # Limit concurrent requests
        
        async def bounded_request(session, service_name, request_id):
            async with semaphore:
                return await self._make_bridge_request(session, service_name, request_id)
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(num_requests):
                service_name = f"test-service-{i % 50}"
                task = bounded_request(session, service_name, i)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    failed += 1
                    errors.append(str(result))
                else:
                    successful += 1
                    latencies.append(result)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        total_time = end_time - start_time
        
        await self._cleanup_test_bridge()
        
        return BenchmarkResult(
            test_name="concurrent_load",
            total_requests=num_requests,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            requests_per_second=successful / total_time if total_time > 0 else 0,
            avg_latency=statistics.mean(latencies) if latencies else 0,
            p95_latency=np.percentile(latencies, 95) if latencies else 0,
            p99_latency=np.percentile(latencies, 99) if latencies else 0,
            memory_usage_mb=end_memory - start_memory,
            cpu_usage_percent=psutil.cpu_percent(interval=1),
            errors=errors[:10]
        )
    
    async def benchmark_memory_efficiency(self, num_requests: int, duration: int) -> BenchmarkResult:
        """Benchmark memory efficiency with sustained load."""
        await self._setup_test_bridge(num_services=100)
        
        memory_samples = []
        latencies = []
        successful = 0
        failed = 0
        
        start_time = time.time()
        
        # Monitor memory during sustained load
        async def memory_monitor():
            while True:
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                memory_samples.append(memory_mb)
                await asyncio.sleep(1)
        
        monitor_task = asyncio.create_task(memory_monitor())
        
        try:
            async with aiohttp.ClientSession() as session:
                for i in range(num_requests):
                    try:
                        service_name = f"test-service-{i % 100}"
                        latency = await self._make_bridge_request(session, service_name, i)
                        latencies.append(latency)
                        successful += 1
                        
                        # Small delay to spread load over time
                        if i % 1000 == 0:
                            await asyncio.sleep(0.1)
                            
                    except Exception as e:
                        failed += 1
        finally:
            monitor_task.cancel()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        await self._cleanup_test_bridge()
        
        return BenchmarkResult(
            test_name="memory_efficiency",
            total_requests=num_requests,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            requests_per_second=successful / total_time if total_time > 0 else 0,
            avg_latency=statistics.mean(latencies) if latencies else 0,
            p95_latency=np.percentile(latencies, 95) if latencies else 0,
            p99_latency=np.percentile(latencies, 99) if latencies else 0,
            memory_usage_mb=max(memory_samples) - min(memory_samples) if memory_samples else 0,
            cpu_usage_percent=psutil.cpu_percent(interval=1),
            errors=[]
        )
    
    async def _make_baseline_request(self, session: aiohttp.ClientSession) -> float:
        """Make a baseline REST request."""
        start_time = time.time()
        
        # Simulate a simple REST call
        await asyncio.sleep(0.005)  # 5ms simulated latency
        
        return time.time() - start_time
    
    async def _make_bridge_request(self, session: aiohttp.ClientSession, service_name: str, request_id: int) -> float:
        """Make a request through the Universal Bridge."""
        start_time = time.time()
        
        try:
            # Make request to bridge
            url = f"{self.bridge_url}/api/{service_name}/test"
            data = {"request_id": request_id, "data": f"test_{request_id}"}
            
            async with session.post(url, json=data, timeout=aiohttp.ClientTimeout(total=5)) as response:
                await response.json()
                
            return time.time() - start_time
            
        except Exception as e:
            raise Exception(f"Bridge request failed: {e}")
    
    async def _setup_test_bridge(self, num_services: int = 10) -> None:
        """Setup test bridge with mock services."""
        config = create_massive_scale_config(num_services)
        
        # Add test service clusters
        for i in range(num_services):
            service_name = f"test-service-{i}"
            endpoints = [f"localhost:{50000 + i}"]
            cluster = create_cluster_from_endpoints(service_name, endpoints)
            config.add_service_cluster(cluster)
        
        self.bridge = UniversalBridge(config)
        # In a real implementation, this would start the bridge
        # For now, we'll simulate it
        print(f"   âœ… Test bridge setup with {num_services} services")
    
    async def _setup_massive_scale_bridge(self, num_services: int) -> None:
        """Setup bridge for massive scale testing."""
        config = create_massive_scale_config(num_services)
        
        # Batch create services for efficiency
        for i in range(0, num_services, 100):
            batch_end = min(i + 100, num_services)
            for j in range(i, batch_end):
                service_name = f"service-{j}"
                endpoints = [f"localhost:{50000 + j}"]
                cluster = create_cluster_from_endpoints(service_name, endpoints)
                config.add_service_cluster(cluster)
        
        self.bridge = UniversalBridge(config)
        print(f"   âœ… Massive scale bridge setup with {num_services} services")
    
    async def _cleanup_test_bridge(self) -> None:
        """Cleanup test bridge."""
        if self.bridge:
            # In real implementation, this would stop the bridge
            self.bridge = None
        print("   ğŸ§¹ Test bridge cleaned up")
    
    def _print_result(self, result: BenchmarkResult) -> None:
        """Print formatted benchmark result."""
        print(f"   ğŸ“ˆ {result.test_name}:")
        print(f"      Requests: {result.successful_requests}/{result.total_requests}")
        print(f"      RPS: {result.requests_per_second:.2f}")
        print(f"      Latency: avg={result.avg_latency*1000:.2f}ms, p95={result.p95_latency*1000:.2f}ms")
        print(f"      Memory: {result.memory_usage_mb:.2f}MB")
        print(f"      CPU: {result.cpu_usage_percent:.1f}%")
        if result.errors:
            print(f"      Errors: {len(result.errors)} (showing first few)")
            for error in result.errors[:3]:
                print(f"        - {error}")
    
    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report."""
        report = {
            "summary": {},
            "performance_comparison": {},
            "detailed_results": [],
            "recommendations": []
        }
        
        # Find baseline and bridge results
        baseline_result = next((r for r in self.results if r.test_name == "baseline_rest"), None)
        bridge_result = next((r for r in self.results if r.test_name == "universal_bridge"), None)
        massive_result = next((r for r in self.results if r.test_name == "massive_scale"), None)
        
        # Calculate performance improvements
        if baseline_result and bridge_result:
            latency_improvement = baseline_result.avg_latency / bridge_result.avg_latency if bridge_result.avg_latency > 0 else 0
            throughput_improvement = bridge_result.requests_per_second / baseline_result.requests_per_second if baseline_result.requests_per_second > 0 else 0
            
            report["performance_comparison"] = {
                "latency_improvement": f"{latency_improvement:.1f}x faster",
                "throughput_improvement": f"{throughput_improvement:.1f}x higher",
                "memory_efficiency": f"{baseline_result.memory_usage_mb / bridge_result.memory_usage_mb:.1f}x more efficient" if bridge_result.memory_usage_mb > 0 else "N/A"
            }
        
        # Summary statistics
        total_requests = sum(r.successful_requests for r in self.results)
        avg_rps = statistics.mean([r.requests_per_second for r in self.results if r.requests_per_second > 0])
        
        report["summary"] = {
            "total_tests": len(self.results),
            "total_requests_processed": total_requests,
            "average_rps": avg_rps,
            "max_services_tested": 10000 if massive_result else 100,
            "performance_targets_met": self._check_performance_targets()
        }
        
        # Detailed results
        for result in self.results:
            report["detailed_results"].append({
                "test": result.test_name,
                "rps": result.requests_per_second,
                "avg_latency_ms": result.avg_latency * 1000,
                "p95_latency_ms": result.p95_latency * 1000,
                "success_rate": result.successful_requests / result.total_requests if result.total_requests > 0 else 0,
                "memory_mb": result.memory_usage_mb,
                "cpu_percent": result.cpu_usage_percent
            })
        
        # Generate recommendations
        report["recommendations"] = self._generate_recommendations()
        
        return report
    
    def _check_performance_targets(self) -> Dict[str, bool]:
        """Check if performance targets are met."""
        targets = {
            "10x_latency_improvement": False,
            "50x_throughput_improvement": False,
            "10k_services_support": False,
            "memory_efficiency": False
        }
        
        baseline_result = next((r for r in self.results if r.test_name == "baseline_rest"), None)
        bridge_result = next((r for r in self.results if r.test_name == "universal_bridge"), None)
        massive_result = next((r for r in self.results if r.test_name == "massive_scale"), None)
        
        if baseline_result and bridge_result:
            if bridge_result.avg_latency > 0:
                latency_improvement = baseline_result.avg_latency / bridge_result.avg_latency
                targets["10x_latency_improvement"] = latency_improvement >= 10
            
            if baseline_result.requests_per_second > 0:
                throughput_improvement = bridge_result.requests_per_second / baseline_result.requests_per_second
                targets["50x_throughput_improvement"] = throughput_improvement >= 50
            
            if bridge_result.memory_usage_mb > 0:
                memory_efficiency = baseline_result.memory_usage_mb / bridge_result.memory_usage_mb
                targets["memory_efficiency"] = memory_efficiency >= 5
        
        if massive_result and massive_result.successful_requests > 0:
            targets["10k_services_support"] = True
        
        return targets
    
    def _generate_recommendations(self) -> List[str]:
        """Generate performance recommendations."""
        recommendations = []
        
        # Analyze results and provide recommendations
        for result in self.results:
            if result.failed_requests > result.successful_requests * 0.05:  # >5% failure rate
                recommendations.append(f"High failure rate in {result.test_name} - consider increasing timeouts or connection pools")
            
            if result.memory_usage_mb > 1000:  # >1GB memory usage
                recommendations.append(f"High memory usage in {result.test_name} - consider optimizing caching or connection management")
            
            if result.cpu_usage_percent > 80:
                recommendations.append(f"High CPU usage in {result.test_name} - consider scaling horizontally")
        
        if not recommendations:
            recommendations.append("All performance metrics look good! System is performing optimally.")
        
        return recommendations


async def main():
    """Run the performance benchmark suite."""
    benchmark = PerformanceBenchmark()
    
    try:
        # Run comprehensive benchmarks
        report = await benchmark.run_full_benchmark_suite()
        
        # Print final report
        print("\n" + "=" * 70)
        print("ğŸ¯ FINAL PERFORMANCE REPORT")
        print("=" * 70)
        
        print(f"\nğŸ“Š Summary:")
        for key, value in report["summary"].items():
            print(f"   {key}: {value}")
        
        print(f"\nğŸš€ Performance Comparison:")
        for key, value in report["performance_comparison"].items():
            print(f"   {key}: {value}")
        
        print(f"\nâœ… Performance Targets:")
        for target, achieved in report["summary"]["performance_targets_met"].items():
            status = "âœ…" if achieved else "âŒ"
            print(f"   {status} {target}: {achieved}")
        
        print(f"\nğŸ’¡ Recommendations:")
        for rec in report["recommendations"]:
            print(f"   â€¢ {rec}")
        
        # Save detailed report
        with open("performance_report.json", "w") as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nğŸ“„ Detailed report saved to: performance_report.json")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Benchmark interrupted by user")
    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main()) 